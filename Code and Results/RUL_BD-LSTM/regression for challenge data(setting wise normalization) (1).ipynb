{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\trg data now.csv\")\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "# train_df.drop(['HI'],axis=1,inplace=True)\n",
    "\n",
    "# rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "# rul.columns = ['id', 'max']\n",
    "# train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "# train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "# train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "test_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\test data.csv\")\n",
    "# test_df.drop(['HI'],axis=1,inplace=True)\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0=train_df[train_df['setting3']==0]\n",
    "min_max_scaler_0 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_0 = pd.DataFrame(min_max_scaler_0.fit_transform(train_df_0[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_0.index)\n",
    "join_df_0 = train_df_0[train_df.columns.difference(cols_normalize)].join(norm_train_df_0)\n",
    "train_df_0 = join_df_0.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_20=train_df[train_df['setting3']==20]\n",
    "min_max_scaler_20 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_20 = pd.DataFrame(min_max_scaler_20.fit_transform(train_df_20[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_20.index)\n",
    "join_df_20 = train_df_20[train_df.columns.difference(cols_normalize)].join(norm_train_df_20)\n",
    "train_df_20 = join_df_20.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_40=train_df[train_df['setting3']==40]\n",
    "min_max_scaler_40 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_40 = pd.DataFrame(min_max_scaler_40.fit_transform(train_df_40[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_40.index)\n",
    "join_df_40 = train_df_40[train_df.columns.difference(cols_normalize)].join(norm_train_df_40)\n",
    "train_df_40 = join_df_40.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_60=train_df[train_df['setting3']==60]\n",
    "min_max_scaler_60 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_60 = pd.DataFrame(min_max_scaler_60.fit_transform(train_df_60[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_60.index)\n",
    "join_df_60 = train_df_60[train_df.columns.difference(cols_normalize)].join(norm_train_df_60)\n",
    "train_df_60 = join_df_60.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_80=train_df[train_df['setting3']==80]\n",
    "min_max_scaler_80 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_80 = pd.DataFrame(min_max_scaler_80.fit_transform(train_df_80[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_80.index)\n",
    "join_df_80 = train_df_80[train_df.columns.difference(cols_normalize)].join(norm_train_df_80)\n",
    "train_df_80 = join_df_80.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_100=train_df[train_df['setting3']==100]\n",
    "min_max_scaler_100 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_100 = pd.DataFrame(min_max_scaler_100.fit_transform(train_df_100[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_100.index)\n",
    "join_df_100 = train_df_100[train_df.columns.difference(cols_normalize)].join(norm_train_df_100)\n",
    "train_df_100 = join_df_100.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in train_df_0.index:\n",
    "    train_df.iloc[index]=train_df_0.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_20.index:\n",
    "    train_df.iloc[index]=train_df_20.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_40.index:\n",
    "    train_df.iloc[index]=train_df_40.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_60.index:\n",
    "    train_df.iloc[index]=train_df_60.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_80.index:\n",
    "    train_df.iloc[index]=train_df_80.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_100.index:\n",
    "    train_df.iloc[index]=train_df_100.iloc[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "test_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\test data.csv\")\n",
    "test_df['cycle_norm'] = test_df['cycle']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_0=test_df[test_df['setting3']==0]\n",
    "norm_test_df_0 = pd.DataFrame(min_max_scaler_0.transform(test_df_0[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_0.index)\n",
    "test_join_df_0 = test_df_0[test_df.columns.difference(cols_normalize)].join(norm_test_df_0)\n",
    "test_df_0 = test_join_df_0.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_20=test_df[test_df['setting3']==20]\n",
    "norm_test_df_20 = pd.DataFrame(min_max_scaler_20.transform(test_df_20[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_20.index)\n",
    "test_join_df_20 = test_df_20[test_df.columns.difference(cols_normalize)].join(norm_test_df_20)\n",
    "test_df_20 = test_join_df_20.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_40=test_df[test_df['setting3']==40]\n",
    "norm_test_df_40 = pd.DataFrame(min_max_scaler_40.transform(test_df_40[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_40.index)\n",
    "test_join_df_40 = test_df_40[test_df.columns.difference(cols_normalize)].join(norm_test_df_40)\n",
    "test_df_40 = test_join_df_40.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_60=test_df[test_df['setting3']==60]\n",
    "norm_test_df_60 = pd.DataFrame(min_max_scaler_60.transform(test_df_60[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_60.index)\n",
    "test_join_df_60 = test_df_60[test_df.columns.difference(cols_normalize)].join(norm_test_df_60)\n",
    "test_df_60 = test_join_df_60.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_80=test_df[test_df['setting3']==80]\n",
    "norm_test_df_80 = pd.DataFrame(min_max_scaler_80.transform(test_df_80[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_80.index)\n",
    "test_join_df_80 = test_df_80[test_df.columns.difference(cols_normalize)].join(norm_test_df_80)\n",
    "test_df_80 = test_join_df_80.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_100=test_df[test_df['setting3']==100]\n",
    "norm_test_df_100 = pd.DataFrame(min_max_scaler_100.transform(test_df_100[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_100.index)\n",
    "test_join_df_100 = test_df_100[test_df.columns.difference(cols_normalize)].join(norm_test_df_100)\n",
    "test_df_100 = test_join_df_100.reindex(columns = test_df.columns)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in test_df_0.index:\n",
    "    test_df.iloc[index]=test_df_0.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_20.index:\n",
    "    test_df.iloc[index]=test_df_20.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_40.index:\n",
    "    test_df.iloc[index]=test_df_40.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_60.index:\n",
    "    test_df.iloc[index]=test_df_60.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_80.index:\n",
    "    test_df.iloc[index]=test_df_80.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_100.index:\n",
    "    test_df.iloc[index]=test_df_100.iloc[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
      "0  1.0    1.0  0.533333      0.10       0.0  0.0  0.501608  0.362961   \n",
      "1  1.0    2.0  0.130000      0.75       0.0  0.0  0.303030  0.435366   \n",
      "2  1.0    3.0  0.133333      0.00       0.0  0.0  0.427653  0.421908   \n",
      "3  1.0    4.0  0.540000      0.00       0.0  0.0  0.301829  0.398041   \n",
      "4  1.0    5.0  0.800000      0.55       0.0  0.0  0.456592  0.322581   \n",
      "\n",
      "         s4   s5  ...       s14       s15  s16       s17  s18  s19       s20  \\\n",
      "0  0.475272  0.0  ...  0.113280  0.446860  0.0  0.500000  0.0  0.0  0.689655   \n",
      "1  0.551189  0.0  ...  0.099372  0.338811  0.0  0.555556  0.0  0.0  0.532468   \n",
      "2  0.424412  0.0  ...  0.096114  0.550322  0.0  0.500000  0.0  0.0  0.637931   \n",
      "3  0.493206  0.0  ...  0.160807  0.530522  0.0  0.400000  0.0  0.0  0.628866   \n",
      "4  0.399509  0.0  ...  0.079537  0.549114  0.0  0.400000  0.0  0.0  0.560345   \n",
      "\n",
      "        s21   HI  cycle_norm  \n",
      "0  0.583852  1.0    0.000000  \n",
      "1  0.761757  1.0    0.002890  \n",
      "2  0.535234  1.0    0.005698  \n",
      "3  0.467611  1.0    0.008475  \n",
      "4  0.597598  1.0    0.011396  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Normalized train with 9 sensors.csv\",index=False)\n",
    "test_df.to_csv(\"Normalized test with 9 sensors.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"Normalized train with 9 sensors.csv\")\n",
    "test_df=pd.read_csv(\"Normalized test with 9 sensors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35018, 50, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35018, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 50\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ....\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s2','s3','s4','s7','s11','s12','s15','s20','s21']\n",
    "# sensor_cols=['s3','s7','s11','s12']\n",
    "# sensor_cols=['s'+str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting3', 'cycle_norm','HI']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
    "# print(len(val))\n",
    "\n",
    "\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 33267 samples, validate on 1751 samples\n",
      "Epoch 1/100\n",
      " - 87s - loss: 7944.7919 - val_loss: 7224.6595\n",
      "Epoch 2/100\n",
      " - 63s - loss: 5659.1091 - val_loss: 5228.4924\n",
      "Epoch 3/100\n",
      " - 56s - loss: 4394.2783 - val_loss: 4120.9855\n",
      "Epoch 4/100\n",
      " - 54s - loss: 3604.7312 - val_loss: 2911.8679\n",
      "Epoch 5/100\n",
      " - 54s - loss: 2643.3776 - val_loss: 2143.3737\n",
      "Epoch 6/100\n",
      " - 56s - loss: 2243.7619 - val_loss: 1669.6060\n",
      "Epoch 7/100\n",
      " - 57s - loss: 2019.2437 - val_loss: 1405.2677\n",
      "Epoch 8/100\n",
      " - 57s - loss: 1909.0019 - val_loss: 1285.5543\n",
      "Epoch 9/100\n",
      " - 56s - loss: 1801.8111 - val_loss: 1173.5788\n",
      "Epoch 10/100\n",
      " - 60s - loss: 1723.7476 - val_loss: 990.6125\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-118d0a047aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m history = model.fit(seq_array, label_array, epochs=100, batch_size=32, validation_split=0.05, verbose=2,\n\u001b[0;32m     26\u001b[0m           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='min'),\n\u001b[1;32m---> 27\u001b[1;33m                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n\u001b[0m\u001b[0;32m     28\u001b[0m           )\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=24,\n",
    "         return_sequences=True)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Bidirectional(LSTM(\n",
    "          units=10,\n",
    "          return_sequences=False)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "\n",
    "# print(model.summary())\n",
    "# model.set_weights(estimator.get_weights())\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=32, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "print(model.summary())\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regression_model.h5'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_regression_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_array_test_last\n",
      "(201, 50, 12)\n"
     ]
    }
   ],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "print(\"seq_array_test_last\")\n",
    "print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "# #print(\"y_mask\")\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "    \n",
    "    y_pred_test = estimator.predict(seq_array_test_last)\n",
    "#     y_pred_test = min_max_scaler_r.inverse_transform(estimator.predict(seq_array_test_last))\n",
    "    y_prediction=[]\n",
    "    index=0\n",
    "    for value in y_mask:\n",
    "        if not value:\n",
    "            y_prediction.append([125])\n",
    "        else:\n",
    "            y_prediction.append(y_pred_test[index])\n",
    "            index+=1\n",
    "    y_pred_test=y_prediction\n",
    "    \n",
    "    test_set = pd.DataFrame(y_pred_test)\n",
    "    test_set.to_csv('submit_test.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test=[]\n",
    "# estimator = load_model(model_path)\n",
    "# for id in test_df['id'].unique():\n",
    "#     x=len(test_df[test_df['id']==id])\n",
    "#     if x>=50:\n",
    "#         y_pred_test.append(estimator.predict(np.asarray([test_df[test_df['id']==id][sequence_cols].values[-50:]]).astype(np.float32))[0][0])\n",
    "#     else:\n",
    "#         y_pred_test.append(estimator.predict(np.asarray([test_df[test_df['id']==id][sequence_cols].values]).astype(np.float32))[0][0])\n",
    "\n",
    "# test_set = pd.DataFrame(y_pred_test)\n",
    "# test_set.to_csv('submit_test.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"submit_test.csv\")\n",
    "writePath=\"submit_test.txt\"\n",
    "with open(writePath, 'w') as f:\n",
    "    f.write(df.to_string(header = False, index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 50, 48)            7104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 48)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20)                4720      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,845\n",
      "Trainable params: 11,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(estimator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
