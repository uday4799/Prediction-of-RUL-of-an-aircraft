{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\trg data now.csv\")\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "test_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\test data.csv\")\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0=train_df[train_df['setting3']==0]\n",
    "min_max_scaler_0 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_0 = pd.DataFrame(min_max_scaler_0.fit_transform(train_df_0[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_0.index)\n",
    "join_df_0 = train_df_0[train_df.columns.difference(cols_normalize)].join(norm_train_df_0)\n",
    "train_df_0 = join_df_0.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_20=train_df[train_df['setting3']==20]\n",
    "min_max_scaler_20 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_20 = pd.DataFrame(min_max_scaler_20.fit_transform(train_df_20[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_20.index)\n",
    "join_df_20 = train_df_20[train_df.columns.difference(cols_normalize)].join(norm_train_df_20)\n",
    "train_df_20 = join_df_20.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_40=train_df[train_df['setting3']==40]\n",
    "min_max_scaler_40 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_40 = pd.DataFrame(min_max_scaler_40.fit_transform(train_df_40[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_40.index)\n",
    "join_df_40 = train_df_40[train_df.columns.difference(cols_normalize)].join(norm_train_df_40)\n",
    "train_df_40 = join_df_40.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_60=train_df[train_df['setting3']==60]\n",
    "min_max_scaler_60 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_60 = pd.DataFrame(min_max_scaler_60.fit_transform(train_df_60[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_60.index)\n",
    "join_df_60 = train_df_60[train_df.columns.difference(cols_normalize)].join(norm_train_df_60)\n",
    "train_df_60 = join_df_60.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_80=train_df[train_df['setting3']==80]\n",
    "min_max_scaler_80 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_80 = pd.DataFrame(min_max_scaler_80.fit_transform(train_df_80[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_80.index)\n",
    "join_df_80 = train_df_80[train_df.columns.difference(cols_normalize)].join(norm_train_df_80)\n",
    "train_df_80 = join_df_80.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_100=train_df[train_df['setting3']==100]\n",
    "min_max_scaler_100 = preprocessing.MinMaxScaler()\n",
    "norm_train_df_100 = pd.DataFrame(min_max_scaler_100.fit_transform(train_df_100[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df_100.index)\n",
    "join_df_100 = train_df_100[train_df.columns.difference(cols_normalize)].join(norm_train_df_100)\n",
    "train_df_100 = join_df_100.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in train_df_0.index:\n",
    "    train_df.iloc[index]=train_df_0.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_20.index:\n",
    "    train_df.iloc[index]=train_df_20.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_40.index:\n",
    "    train_df.iloc[index]=train_df_40.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_60.index:\n",
    "    train_df.iloc[index]=train_df_60.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_80.index:\n",
    "    train_df.iloc[index]=train_df_80.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in train_df_100.index:\n",
    "    train_df.iloc[index]=train_df_100.iloc[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "test_df=pd.read_csv(r\"C:\\Users\\Uday\\Desktop\\BTP\\test data.csv\")\n",
    "test_df['cycle_norm'] = test_df['cycle']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_0=test_df[test_df['setting3']==0]\n",
    "norm_test_df_0 = pd.DataFrame(min_max_scaler_0.transform(test_df_0[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_0.index)\n",
    "test_join_df_0 = test_df_0[test_df.columns.difference(cols_normalize)].join(norm_test_df_0)\n",
    "test_df_0 = test_join_df_0.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_20=test_df[test_df['setting3']==20]\n",
    "norm_test_df_20 = pd.DataFrame(min_max_scaler_20.transform(test_df_20[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_20.index)\n",
    "test_join_df_20 = test_df_20[test_df.columns.difference(cols_normalize)].join(norm_test_df_20)\n",
    "test_df_20 = test_join_df_20.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_40=test_df[test_df['setting3']==40]\n",
    "norm_test_df_40 = pd.DataFrame(min_max_scaler_40.transform(test_df_40[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_40.index)\n",
    "test_join_df_40 = test_df_40[test_df.columns.difference(cols_normalize)].join(norm_test_df_40)\n",
    "test_df_40 = test_join_df_40.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_60=test_df[test_df['setting3']==60]\n",
    "norm_test_df_60 = pd.DataFrame(min_max_scaler_60.transform(test_df_60[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_60.index)\n",
    "test_join_df_60 = test_df_60[test_df.columns.difference(cols_normalize)].join(norm_test_df_60)\n",
    "test_df_60 = test_join_df_60.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_80=test_df[test_df['setting3']==80]\n",
    "norm_test_df_80 = pd.DataFrame(min_max_scaler_80.transform(test_df_80[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_80.index)\n",
    "test_join_df_80 = test_df_80[test_df.columns.difference(cols_normalize)].join(norm_test_df_80)\n",
    "test_df_80 = test_join_df_80.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_100=test_df[test_df['setting3']==100]\n",
    "norm_test_df_100 = pd.DataFrame(min_max_scaler_100.transform(test_df_100[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df_100.index)\n",
    "test_join_df_100 = test_df_100[test_df.columns.difference(cols_normalize)].join(norm_test_df_100)\n",
    "test_df_100 = test_join_df_100.reindex(columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in test_df_0.index:\n",
    "    test_df.iloc[index]=test_df_0.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_20.index:\n",
    "    test_df.iloc[index]=test_df_20.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_40.index:\n",
    "    test_df.iloc[index]=test_df_40.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_60.index:\n",
    "    test_df.iloc[index]=test_df_60.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_80.index:\n",
    "    test_df.iloc[index]=test_df_80.iloc[i]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for index in test_df_100.index:\n",
    "    test_df.iloc[index]=test_df_100.iloc[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Normalized train with 9 sensors.csv\",index=False)\n",
    "test_df.to_csv(\"Normalized test with 9 sensors.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"Normalized train with 9 sensors.csv\")\n",
    "test_df=pd.read_csv(\"Normalized test with 9 sensors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ....\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s2','s3','s4','s7','s11','s12','s15','s20','s21']\n",
    "sequence_cols = ['setting3', 'cycle_norm','HI']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 24 units followed by another LSTM layer with 10 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=24,\n",
    "         return_sequences=True)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Bidirectional(LSTM(\n",
    "          units=10,\n",
    "          return_sequences=False)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=32, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "print(model.summary())\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating mse plot of train and validation set\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_regression_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "print(\"seq_array_test_last\")\n",
    "print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "# #print(\"y_mask\")\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "    \n",
    "    y_pred_test = estimator.predict(seq_array_test_last)\n",
    "#     y_pred_test = min_max_scaler_r.inverse_transform(estimator.predict(seq_array_test_last))\n",
    "    y_prediction=[]\n",
    "    index=0\n",
    "    for value in y_mask:\n",
    "        if not value:\n",
    "            y_prediction.append([125])\n",
    "        else:\n",
    "            y_prediction.append(y_pred_test[index])\n",
    "            index+=1\n",
    "    y_pred_test=y_prediction\n",
    "    \n",
    "    test_set = pd.DataFrame(y_pred_test)\n",
    "    test_set.to_csv('submit_test.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results(submit_test.txt) will be saved in the current directory\n",
    "df=pd.read_csv(r\"submit_test.csv\")\n",
    "writePath=\"submit_test.txt\"\n",
    "with open(writePath, 'w') as f:\n",
    "    f.write(df.to_string(header = False, index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
